{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T15:55:16.693027100Z",
     "start_time": "2024-06-07T15:55:01.807596500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from bertopic import BERTopic\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import jupyter\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis"
   ],
   "id": "ae9faf1a06662a0e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T18:29:02.490825Z",
     "start_time": "2024-05-15T18:29:02.487361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download NLTK resources if not already downloaded\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ],
   "id": "b7d74a135301bf4a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hmuen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hmuen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hmuen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T18:29:02.508277Z",
     "start_time": "2024-05-15T18:29:02.504928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocess stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "specific_words = {\"br\", \"positive\", \"negative\"}\n",
    "stop_words.update(specific_words)"
   ],
   "id": "8ab2e171446311e8",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T18:29:02.512785Z",
     "start_time": "2024-05-15T18:29:02.509282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove punctuation and stopwords\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return tokens"
   ],
   "id": "c7a77a80ef34e2a8",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T18:29:02.974020Z",
     "start_time": "2024-05-15T18:29:02.512785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read data from CSV file\n",
    "csv_file = r'C:\\Users\\hmuen\\PycharmProjects\\imdb_analysis\\data\\IMDB Dataset.csv'\n",
    "df = pd.read_csv(csv_file)"
   ],
   "id": "baf73ce2e62c582b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T18:29:02.978037Z",
     "start_time": "2024-05-15T18:29:02.974020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert reviews to list\n",
    "texts = df[\"review\"].tolist()"
   ],
   "id": "c6614fdada880d81",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T18:30:02.715567Z",
     "start_time": "2024-05-15T18:29:02.978037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocess documents\n",
    "preprocessed_documents = [preprocess_text(text) for text in texts]"
   ],
   "id": "66858a7c80a8561f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T18:30:11.384262Z",
     "start_time": "2024-05-15T18:30:02.716573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "topic_model = BERTopic()"
   ],
   "id": "5a707105aaed7ffd",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T18:30:27.158902Z",
     "start_time": "2024-05-15T18:30:11.384769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train LDA model\n",
    "num_topics = 10  # You can adjust the number of topics as per your requirement\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)"
   ],
   "id": "c66e26ca6168b32b",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Train LDA model\u001B[39;00m\n\u001B[0;32m      2\u001B[0m num_topics \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m  \u001B[38;5;66;03m# You can adjust the number of topics as per your requirement\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m lda_model \u001B[38;5;241m=\u001B[39m \u001B[43mmodels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLdaModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorpus\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_topics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_topics\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mid2word\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdictionary\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpasses\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Lib\\site-packages\\gensim\\models\\ldamodel.py:521\u001B[0m, in \u001B[0;36mLdaModel.__init__\u001B[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001B[0m\n\u001B[0;32m    519\u001B[0m use_numpy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatcher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    520\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m--> 521\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorpus\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunks_as_numpy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_numpy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_lifecycle_event(\n\u001B[0;32m    523\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcreated\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    524\u001B[0m     msg\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrained \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtime\u001B[38;5;241m.\u001B[39mtime()\u001B[38;5;250m \u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;250m \u001B[39mstart\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124ms\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    525\u001B[0m )\n",
      "File \u001B[1;32m~\\Lib\\site-packages\\gensim\\models\\ldamodel.py:1006\u001B[0m, in \u001B[0;36mLdaModel.update\u001B[1;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001B[0m\n\u001B[0;32m   1001\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1002\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\n\u001B[0;32m   1003\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPROGRESS: pass \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m, at document #\u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1004\u001B[0m         pass_, chunk_no \u001B[38;5;241m*\u001B[39m chunksize \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlen\u001B[39m(chunk), lencorpus\n\u001B[0;32m   1005\u001B[0m     )\n\u001B[1;32m-> 1006\u001B[0m     gammat \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_estep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mother\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1008\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimize_alpha:\n\u001B[0;32m   1009\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_alpha(gammat, rho())\n",
      "File \u001B[1;32m~\\Lib\\site-packages\\gensim\\models\\ldamodel.py:768\u001B[0m, in \u001B[0;36mLdaModel.do_estep\u001B[1;34m(self, chunk, state)\u001B[0m\n\u001B[0;32m    766\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m state \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    767\u001B[0m     state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\n\u001B[1;32m--> 768\u001B[0m gamma, sstats \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minference\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollect_sstats\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    769\u001B[0m state\u001B[38;5;241m.\u001B[39msstats \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m sstats\n\u001B[0;32m    770\u001B[0m state\u001B[38;5;241m.\u001B[39mnumdocs \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m gamma\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# avoids calling len(chunk) on a generator\u001B[39;00m\n",
      "File \u001B[1;32m~\\Lib\\site-packages\\gensim\\models\\ldamodel.py:722\u001B[0m, in \u001B[0;36mLdaModel.inference\u001B[1;34m(self, chunk, collect_sstats)\u001B[0m\n\u001B[0;32m    720\u001B[0m Elogthetad \u001B[38;5;241m=\u001B[39m dirichlet_expectation(gammad)\n\u001B[0;32m    721\u001B[0m expElogthetad \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mexp(Elogthetad)\n\u001B[1;32m--> 722\u001B[0m phinorm \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexpElogthetad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexpElogbetad\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m epsilon\n\u001B[0;32m    723\u001B[0m \u001B[38;5;66;03m# If gamma hasn't changed much, we're done.\u001B[39;00m\n\u001B[0;32m    724\u001B[0m meanchange \u001B[38;5;241m=\u001B[39m mean_absolute_difference(gammad, lastgamma)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T18:30:27.160417Z",
     "start_time": "2024-05-15T18:30:27.160417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print topics\n",
    "for topic in lda_model.print_topics():\n",
    "    print(topic)"
   ],
   "id": "ab4d7b03ac9821f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T18:30:27.161417Z",
     "start_time": "2024-05-15T18:30:27.161417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Visualize the LDA model results\n",
    "vis_data = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)"
   ],
   "id": "527f9eb76eb24311",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [],
   "id": "df86934ddd4f22ee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
